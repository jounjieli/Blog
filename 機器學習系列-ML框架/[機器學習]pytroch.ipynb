{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pytorch docs](https://pytorch.org/docs/stable/index.html)\n",
    "# pytorch #\n",
    "## Tensor(張量) ##\n",
    "[Tensor 屬性](https://pytorch.org/docs/stable/tensor_attributes.html#tensor-attributes)\n",
    "pytorch和tensorflow差不多，基本上數據都以Tensor建構。\n",
    "![Tensor類型](https://upload-images.jianshu.io/upload_images/13539817-920cb6072d807598.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "\n",
    "## 基本操作 ##\n",
    "Python本身是一門高級語言，使用很方便，但這也意味著很多操作很低效。\n",
    "實際使用中應盡量調用內建函數(buildin-function)，這些函數底層由C/C++實現，能通過執行底層優化實現高效計算。因此在平時寫代碼時，就應養成向量化的思維習慣，千萬避免對較大的tensor進行逐元素遍歷。\n",
    "\n",
    "- tensor      \n",
    "[tensor操作參考](https://pytorch.org/docs/stable/torch.html#module-torch)，基本上很多操作以及名稱都與numpy差不多，學過numpy的話應該都不是問題。\n",
    "pytorch中如果後綴_的運算或操作為in-place(就地操作)，依照原tensor只修改有改變的相關的屬性且共用同一塊內存。\n",
    "pytorch可以與numpy相互轉換，且轉換後共用同一塊內存，修改時會一起改變，不過存在GPU(cuda:0、cuda:1...)的資料不能轉成numpy。\n",
    "\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-7be83aef199d8821.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-5c10880bc046c88c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-99fc5f19fc186267.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-7b721c03dbe700c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-8fbacf28f9ed3d22.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-71923be6ff10f917.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-74ea4ac1b41fa40d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "## backpropagation ##\n",
    "這邊看一個官方代碼，這是手動實現反向傳播，但既然使用tensorflow、pytorch還要手動微分求導就太麻煩了，而pytorch使用autograd來實現自動求導。\n",
    "```\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "```\n",
    "\n",
    "#### Computational Graphs ####\n",
    "[Calculus on Computational Graphs: Backpropagation](http://colah.github.io/posts/2015-08-Backprop/)\n",
    "[Tree](http://www.csie.ntnu.edu.tw/~u91029/Tree.html)\n",
    "進入autograd 之前建議稍微先了解一下計算圖，如同tensorflow的Tensorboard所輸出的計算圖，但pytorch本身目前1.0是沒有支援計算圖可視化的(之後應該會新增)，不過pytorch是動態建立計算圖的，所以debug上沒有可視化也沒甚麼太大問題。\n",
    "計算圖(Graph)由Tensor所構成的節點(node)所組成，沒有input的節點為leaf(葉子)，如果只有一個初始節點可以稱這個節點為root(根)，將他們連結的稱為邊(edge)。\n",
    "![Tensorboard](https://upload-images.jianshu.io/upload_images/13539817-9615a1c154503ac4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![graph](https://upload-images.jianshu.io/upload_images/13539817-30f1ef5fe6ccb3a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "#### autograd ####\n",
    "autograd 有幾個重點:\n",
    "1. 當graph存在operation node(運算節點)時就會分配buffers(緩衝區)用來存取運算的intermediary(中間結果)(如$\\frac{dz}{dy} = 2y = 44$，$\\frac{dy}{dx} = 4x+1 = 13$，$\\frac{dz}{dy} \\cdot \\frac{dy}{dx} = 572$...)。\n",
    "然而某些運算並不需要建立buffers，ex.add、sub...。\n",
    "如果f(x) = x + w那麼df/dw是1，在這種情況下，不需要建立buffers。\n",
    "如果f(x) = x * w那麼df/dw是x，在這種情況下，我們需要建立buffers。\n",
    "2. 當我們設置tensor的requires_grad=True時，表示這個node需要求導，它的所有衍生節點接為requires_grad=True。\n",
    "3. backward執行時會以執行節點進行反向傳播運算，計算後會將derivative的值傳入requires_grad=True的leaf節點，然後將buffers的intermediary清除以節省內存。\n",
    "4. \"\"requires_grad=True的leaf node\"\" 以及 \"\"需要被用於計算intermediary(中間結果)的node\"\"不可以使用in-place(就地操作)。\n",
    "5. with torch.no_grad:中的所有操作視為requires_grad=False，就算requires_grad設置為True依然忽略為False。\n",
    "6. detach()會傳回leaf的tensor，grad_fn會等於none，is_leaf為True。\n",
    "\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-d5d6ba981b4dc4e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "- 基本操作      \n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-0405f4d502d5c435.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-09989179cd5dc0af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-e9ea86bb2243acf5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "- 取得中間層grad       \n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-c7aaabcf7353f7ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "- 更新權重       \n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-965eb52f5f109ffc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-4b1b0592ac1f5f65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-7767f179d7e849c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-f4541a02fd26f5b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "#### Extending PyTorch ####\n",
    "[Extending PyTorch 官方文檔](https://pytorch.org/docs/stable/notes/extending.html#extending-pytorch)\n",
    "[torch.autograd.Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
    "\n",
    "- 官方 範例1：擴展一個LinearFunction\n",
    ">這邊解釋一下操作:\n",
    ">\n",
    ">$LinearFunction = y = input*weight+bias$\n",
    ">$nextFunction= z = f(LinearFunction )$\n",
    ">1. grad_output = dz/dy\n",
    ">2. dz/dx = dz/dy * dy/dx = grad_output*dy/dx = grad_output*w\n",
    ">3. dz/dw = dz/dy * dy/dw = grad_output*dy/dw = grad_output*x\n",
    ">4. dz/db = dz/dy * dy/db = grad_output*1\n",
    ">- saved_tensors由上下文管理器存取，saved_tensors由上下文管理器提取出來。\n",
    ">- ctx.needs_input_grad作為bool tuple，表示每個輸入是否需要grad。\n",
    ">- 如果第一個輸入到 forward() 的參數需要grad的話，ctx.needs_input_grad[0] = True。\n",
    "\n",
    "\n",
    "```\n",
    "# Inherit from Function\n",
    "class LinearFunction(Function):\n",
    "\n",
    "    # 注意: forward 與 backward 都是靜態的(@staticmethods)\n",
    "    @staticmethod\n",
    "    # bias is an optional(可選) argument\n",
    "   #它必須接受上下文ctx作為第一個參數，後跟任意數量的參數（張量或其他類型）。\n",
    "   #上下文可用於存儲張量，然後可在後向傳遞期間檢索張量。\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "    #unsqueeze(n維前加一個維度)，expand_as(tensor)擴展維與tensor相同形狀\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    #它必須接受一個上下文ctx作為第一個參數,grad_output是第2個參數\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "```\n",
    "\n",
    "- 官方 範例2：擴展一個Exp\n",
    "\n",
    "```\n",
    "class Exp(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i.exp()\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        result, = ctx.saved_tensors\n",
    "        return grad_output * result\n",
    "```\n",
    "\n",
    "- 官方教程 範例3：擴展一個ReLU\n",
    "\n",
    "```\n",
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
