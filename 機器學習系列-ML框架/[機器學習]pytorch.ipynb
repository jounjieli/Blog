{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch #\n",
    "## 官方文檔 ##\n",
    "[pytorch docs](https://pytorch.org/docs/stable/index.html)\n",
    "## 安裝 ##\n",
    "建議下載anaconda創建一個新的環境(env)`conda create -n pytorch_1 python=3.6`，創建好後可以繳活環境`activate pytorch_1 `，然後直接使用官網給的指令安裝(ex.`conda install pytorch torchvision cudatoolkit=10.0 -c pytorch`)，沒有支持CUDA的GPU可以選None。\n",
    " \n",
    "如果要使用jupyter notebook可以`conda install nb_conda`用於notebook自動關聯conda，相較於tensorflow，基本上pytorch安裝不太會有CUDA以及cuDNN版本衝突的問題。\n",
    " \n",
    "也可以參考網路上其他教學:\n",
    "\n",
    "[windows10下安装GPU版pytorch简明教程](https://zhuanlan.zhihu.com/p/54350088)\n",
    "\n",
    "## Tensor(張量) ##\n",
    "[Tensor 屬性](https://pytorch.org/docs/stable/tensor_attributes.html#tensor-attributes)\n",
    "pytorch和tensorflow差不多，基本上數據都以Tensor建構。\n",
    "![Tensor類型](https://upload-images.jianshu.io/upload_images/13539817-920cb6072d807598.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "\n",
    "## 基本操作 ##\n",
    "Python本身是一門高級語言，使用很方便，但這也意味著很多操作很低效。\n",
    "實際使用中應盡量調用內建函數(buildin-function)，這些函數底層由C/C++實現，能通過執行底層優化實現高效計算。因此在平時寫代碼時，就應養成向量化的思維習慣，千萬避免對較大的tensor進行逐元素遍歷。\n",
    "\n",
    "- tensor      \n",
    "[tensor操作參考](https://pytorch.org/docs/stable/torch.html#module-torch)，基本上很多操作以及名稱都與numpy差不多，學過numpy的話應該都不是問題。\n",
    "pytorch中如果後綴_的運算或操作為in-place(就地操作)，依照原tensor只修改有改變的相關的屬性且共用同一塊內存。\n",
    "pytorch可以與numpy相互轉換，且轉換後共用同一塊內存，修改時會一起改變，不過存在GPU(cuda:0、cuda:1...)的資料不能轉成numpy。\n",
    "\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-7be83aef199d8821.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-5c10880bc046c88c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-99fc5f19fc186267.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-7b721c03dbe700c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-8fbacf28f9ed3d22.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-71923be6ff10f917.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-74ea4ac1b41fa40d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "## backpropagation ##\n",
    "#### 簡介 ####\n",
    "這邊看一個官方代碼，這是手動實現反向傳播，但既然使用tensorflow、pytorch還要手動微分求導就太麻煩了，而pytorch使用autograd來實現自動求導。\n",
    "```\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "```\n",
    "\n",
    "#### Computational Graphs ####\n",
    "[Calculus on Computational Graphs: Backpropagation](http://colah.github.io/posts/2015-08-Backprop/)\n",
    "[Tree](http://www.csie.ntnu.edu.tw/~u91029/Tree.html)\n",
    "進入autograd 之前建議稍微先了解一下計算圖，如同tensorflow的Tensorboard所輸出的計算圖，但pytorch本身目前1.0是沒有支援計算圖可視化的(之後應該會新增)，不過pytorch是動態建立計算圖的，所以debug上沒有可視化也沒甚麼太大問題。\n",
    "計算圖(Graph)由Tensor所構成的節點(node)所組成，沒有input的節點為leaf(葉子)，如果只有一個初始節點可以稱這個節點為root(根)，將他們連結的稱為邊(edge)。\n",
    "![Tensorboard](https://upload-images.jianshu.io/upload_images/13539817-9615a1c154503ac4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![graph](https://upload-images.jianshu.io/upload_images/13539817-30f1ef5fe6ccb3a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "#### autograd ####\n",
    "autograd 有幾個重點:\n",
    "1. 當graph存在operation node(運算節點)時就會分配buffers(緩衝區)用來存取運算的intermediary(中間結果)(如$\\frac{dz}{dy} = 2y = 44$，$\\frac{dy}{dx} = 4x+1 = 13$，$\\frac{dz}{dy} \\cdot \\frac{dy}{dx} = 572$...)。\n",
    "然而某些運算並不需要建立buffers，ex.add、sub...。\n",
    "如果f(x) = x + w那麼df/dw是1，在這種情況下，不需要建立buffers。\n",
    "如果f(x) = x * w那麼df/dw是x，在這種情況下，我們需要建立buffers。\n",
    "2. 當我們設置tensor的requires_grad=True時，表示這個node需要求導，它的所有衍生節點接為requires_grad=True。\n",
    "3. backward執行時會以執行節點進行反向傳播運算，計算後會將derivative的值傳入requires_grad=True的leaf節點，然後將buffers的intermediary清除以節省內存。\n",
    "4. \"\"requires_grad=True的leaf node\"\" 以及 \"\"需要被用於計算intermediary(中間結果)的node\"\"不可以使用in-place(就地操作)。\n",
    "若使用了在backward計算到這node會產生Exception，若直接取代變量不影響backward計算(backward計算是指向內存地址) 。\n",
    "5. with torch.no_grad:中的所有操作視為requires_grad=False，就算requires_grad設置為True依然忽略為False。\n",
    "6. detach()會傳回leaf的tensor，grad_fn會等於none，is_leaf為True。\n",
    "\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-d5d6ba981b4dc4e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "- 基本操作      \n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-0405f4d502d5c435.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-09989179cd5dc0af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-e9ea86bb2243acf5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "- 取得中間層grad       \n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-c7aaabcf7353f7ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "- 更新權重       \n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-965eb52f5f109ffc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-4b1b0592ac1f5f65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-7767f179d7e849c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-f4541a02fd26f5b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "## 神經網絡(torch.nn) ##\n",
    "torch.nn與tensorflow.nn以及layer類似，torch.nn裡頭封裝實現了許多神經網絡常用的運算(ex.convolution、activate function、loss function)，使我們可以更輕鬆的建立神經網絡的架構。\n",
    "nn 與 nn.functional兩個是差不多的，不過一個包裝好的類，一個是可以直接調用的函數。\n",
    "\n",
    "#### nn.model ####\n",
    ">使用`model.parameters()`取得參數(parameters)，`parameters()`會傳回一個generator(生成器) 。\n",
    ">我們可以`next()`、`iter()`、`enumerate()`、`list()`，順序會依建立model的順序排序，也可以print(model)查看。\n",
    ">另外`named_parameters()`還會附帶參數名稱，`state_dict()`會產生一個有序字典，可以做`.key()`取得參數名稱、`.values()`取得參數值....等等字典的操作。\n",
    "\n",
    ">model.train（）＃把模設設成訓練模式，影響Dropout和BatchNorm\n",
    ">model.eval（）＃把模塊設置為預測模式，影響Dropout和BatchNorm\n",
    "\n",
    "- 官方 範例1：\n",
    ">使用torch.nn.Sequential建立一個model(類似Keras)。\n",
    ">pytorch的conv與tensorflow默認NHWC不同，是採用nSamples x nChannels x Height x Width作為輸入尺寸。\n",
    "\n",
    "```\n",
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "```\n",
    "\n",
    "- 官方 範例2：\n",
    "\n",
    ">這裡我們[繼承](http://www.runoob.com/python3/python3-class.html)model類，必須先[呼叫父類](http://www.runoob.com/python/python-func-super.html) \\_\\_init\\_\\_，於\\_\\_init\\_\\_中所建構的參數(parameters)，可以被model.parameters()取得。\n",
    "\n",
    ">使用torch.nn的類建構的架構會生成一個類，並且會建立需要的參數(parameters)存於類(class)中，我們實例化後可以使用屬性取得權重或偏移值(ex. model.conv1.weight)。\n",
    ">torch.nn的類有定義\\_\\_call\\_\\_ ，我們可以給類輸入input(必須是tensor)，會return結果(也是一個tensor)，我們可以用來定義前向傳播(forward)。\n",
    "\n",
    "---\n",
    ">我們也可以自行建立parameters，然後於forward中使用torch.nn.functional定義前向傳播(ex. torch.nn.functional.Conv2d)。\n",
    "\n",
    "```\n",
    "class Linear(nn.Module): \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__() # 呼叫父類__init__\n",
    "        self.w = nn.Parameter(t.randn(in_features, out_features))\n",
    "        self.b = nn.Parameter(t.randn(out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.mm(self.w) # x.@(self.w)\n",
    "        return x + self.b.expand_as(x)\n",
    "```\n",
    "---\n",
    "\n",
    "建立模型：\n",
    "```\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "```\n",
    "初始化(__init__)：\n",
    "```\n",
    "net = Net()\n",
    "print(net)\n",
    "```\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-e140691036ab3a6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "觀查模型參數：\n",
    "```\n",
    "params = list(net.parameters())\n",
    "print('params number:',len(params))\n",
    "print(params[0].size())  # conv1's .weight\n",
    "print(params[1].size())  # conv1's .bais\n",
    "print(params[2].size())  # conv2's .weight\n",
    "```\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-0fbe1337b4822588.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "計算前向傳播(forward)：\n",
    "```\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)\n",
    "```\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-2a8459215369e87f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "```\n",
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "```\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-b7f67bf99fab565a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "- 計算反向傳播(backward)\n",
    "\n",
    "```\n",
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)\n",
    "```\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-1d0bca680850af1f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "- 更新參數\n",
    "\n",
    "```\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "```\n",
    "\n",
    "- 範例3   \n",
    "[code](https://gist.github.com/jounjieli/80b1fc92f2280063527a69115dd0f85a)\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## 優化器(torch.optim) ##\n",
    "[torch.optim](https://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "優化器與tensorflow差不多，預先設置優化器，然後使用`optimizer.step()`更新參數。\n",
    "\n",
    "- 範例1：  \n",
    "\n",
    "```\n",
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers(取代前面範例的net.zero_grad())\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update(取代前面範例的更新參數for迴圈)\n",
    "```\n",
    "\n",
    "- 範例2：利用字典分別設定不同parameters的學習率，這蠻方便的，比tensorflow易用許多。\n",
    "\n",
    "```\n",
    "optim.SGD([\n",
    "                {'params': model.base.parameters()},\n",
    "                {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "            ], lr=1e-2, momentum=0.9)\n",
    "```\n",
    "\n",
    "#### Extending PyTorch(擴展pytorch) ####\n",
    "[Extending PyTorch 官方文檔](https://pytorch.org/docs/stable/notes/extending.html#extending-pytorch)\n",
    "\n",
    "[torch.autograd.Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
    "\n",
    "- 官方 範例1：擴展一個LinearFunction\n",
    ">這邊解釋一下操作:\n",
    ">$LinearFunction = y = input*weight+bias$\n",
    ">$nextFunction= z = f(LinearFunction )$\n",
    ">1. grad_output = dz/dy\n",
    ">2. dz/dx = dz/dy * dy/dx = grad_output*dy/dx = grad_output*w\n",
    ">3. dz/dw = dz/dy * dy/dw = grad_output*dy/dw = grad_output*x\n",
    ">4. dz/db = dz/dy * dy/db = grad_output*1\n",
    ">- saved_tensors由上下文管理器存取，saved_tensors由上下文管理器提取出來。\n",
    ">- ctx.needs_input_grad作為bool tuple，表示每個輸入是否需要grad。\n",
    ">- 如果第一個輸入到 forward() 的參數需要grad的話，ctx.needs_input_grad[0] = True。\n",
    "\n",
    "```\n",
    "# Inherit from Function\n",
    "class LinearFunction(Function):\n",
    "\n",
    "    # 注意: forward 與 backward 都是靜態的(@staticmethods)\n",
    "    @staticmethod\n",
    "    # bias is an optional(可選) argument\n",
    "   #它必須接受上下文ctx作為第一個參數，後跟任意數量的參數（張量或其他類型）。\n",
    "   #上下文可用於存儲張量，然後可在後向傳遞期間檢索張量。\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "    #unsqueeze(n維前加一個維度)，expand_as(tensor)擴展維與tensor相同形狀\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    #它必須接受一個上下文ctx作為第一個參數,grad_output是第2個參數\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "```\n",
    "- 官方 範例2：擴展一個Exp\n",
    "```\n",
    "class Exp(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i.exp()\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        result, = ctx.saved_tensors\n",
    "        return grad_output * result\n",
    "```\n",
    "\n",
    "- 官方教程 範例3：擴展一個ReLU\n",
    "\n",
    "```\n",
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "```\n",
    "\n",
    "## 圖形及數據轉換(torchvision.transforms) ##\n",
    "[torchvision.transforms](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision-transforms)\n",
    "\n",
    "[torchvision.transforms.functional](https://pytorch.org/docs/stable/torchvision/transforms.html#functional-transforms)\n",
    "\n",
    "\n",
    "torchvision.transforms提供了很多圖形及數據轉換的方法，用於數據的前處理，可以使用`torchvision.transforms.Compose()`將它們組合在一起。\n",
    "\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-98bd589f5f93a072.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "\n",
    "## 自定義Dataset ##\n",
    "[torchvision.datasets.CIFAR10](https://pytorch.org/docs/stable/_modules/torchvision/datasets/cifar.html#CIFAR10)\n",
    "\n",
    "[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html#DataLoader)\n",
    "\n",
    "[Training a Classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py)\n",
    "\n",
    "`torch.utils.data.Dataset`是一個抽象類，我們可以使用DataLoader類的\\_\\_iter\\_\\_方法將資料依照batch_size生成iter或enumerate，並且可以做shuffle(將數據隨機打亂)以及num_workers(使用多線程來讀數據)。\n",
    "\n",
    "#### Tensor轉Dataset ####\n",
    "我們可以使用torch.utils.data.TensorDataset(*tensor)將資料簡單的轉成Dataset ，參數可輸入多個tensor。\n",
    "\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-007f478d50f7fa61.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "#### 定義torch.utils.data.Dataset ####\n",
    "自定義的Dataset需要繼承它並且實現兩個成員方法：\n",
    "1. \\_\\_getitem\\_\\_()   \n",
    "第一個傳入參數必須是index用來表示要取得第幾筆資料，return必須是 tensors, numbers, dicts, lists or numpy，但DataLoader都會自動處理成tensor。\n",
    "2. \\_\\_len\\_\\_()\n",
    "return資料的長度\n",
    "3. \\_\\_init\\_\\_()   \n",
    "初始化也可不實現，但建議資料整理在init先整理好，getitem會在iter取值時才呼叫，所以會造成每次batch取值都要先call一段很長的getitem，會造成訓練時間增長，建議先在init先花時間整理好。\n",
    "\n",
    "\n",
    "- 範例1      \n",
    "可以參考官方CIFAR10源碼[torchvision.datasets.CIFAR10](https://pytorch.org/docs/stable/_modules/torchvision/datasets/cifar.html#CIFAR10)如何將cifar-10-python.tar.gz整理成Dataset。\n",
    "\n",
    "- 範例2      \n",
    ">下面資料是之前將cifar10整理成的.npy(numpy檔案格式)。\n",
    ">feature資料格式是NHWC且做min max normalization[0 to 1]，labels資料格式是one->hot-encoding。\n",
    ">0~5共60000筆，7是經過旋轉的增量資料，noise是做autoencoder加過噪音的資料，我>們將使用無噪音這些資料來定義一個Dataset。\n",
    "![資料](https://upload-images.jianshu.io/upload_images/13539817-539bd6657988135a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "__cifar-10轉npy(需要先將.gz解壓縮)：__\n",
    "```\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "def cifar_10_to_npy():\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    save_path = r'D:\\ml_data\\data_set\\cifar-10-python.tar'\n",
    "    cifar_dir_path = r'D:\\ml_data\\data_set\\cifar-10-python.tar\\cifar-10-batches-py'\n",
    "    cifar_file_name = \"data_batch_\"\n",
    "    cifar_path = os.path.join(cifar_dir_path,cifar_file_name)\n",
    "    for num in range(1,6):\n",
    "        data = unpickle(cifar_path+str(num))[b'data']\n",
    "        feature = np.transpose((np.array(data)/255).astype(np.float32).reshape(-1,3,32,32),[0,2,3,1])\n",
    "        np.save(os.path.join(save_path,'feature_')+str(num-1)+'.npy',feature)\n",
    "        label_ = unpickle(cifar_path+str(num))[b'labels']\n",
    "        label = np.array(pd.get_dummies(label_))\n",
    "        np.save(os.path.join(save_path,'label_')+str(num-1)+'.npy',label)\n",
    "    data = unpickle(os.path.join(cifar_dir_path,'test_batch'))[b'data']\n",
    "    feature = np.transpose((np.array(data)/255).astype(np.float32).reshape(-1,3,32,32),[0,2,3,1])      \n",
    "    np.save(os.path.join(save_path,'feature_5')+'.npy',feature)\n",
    "    label_ = unpickle(os.path.join(cifar_dir_path,'test_batch'))[b'labels']\n",
    "    label = np.array(pd.get_dummies(label_))\n",
    "    np.save(os.path.join(save_path,'label_')+'.npy',label)\n",
    "cifar_10_to_npy()\n",
    "```\n",
    "\n",
    "__在windows多線程可能會有問題，建議將寫好的class存成.py然後import進來，或者設置num_workers為零不使用多線程。__\n",
    "__這邊為了展示DataLoader會將getitem的return處理成tensor，所以寫了一個my_transforms將x(img)轉成tensor設置好device，然後y(target)維持ndarray，建議最後next取值後在以to轉換，效能較優。__\n",
    "```\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import torchvision.transforms as transforms\n",
    "class Custom_Cifar10(data.Dataset):\n",
    "    classes_name = ['plane', 'car', 'bird', 'cat','deer',\n",
    "                    'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    def __init__(self,transform=None,target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        #迭代0~5筆資料依特徵及標籤分別存入data,label列表\n",
    "        for num in range(6):\n",
    "            X,Y = self.get_data_cifar10(num)\n",
    "            self.data.append(X)\n",
    "            self.targets.append(Y)\n",
    "        #列表ndarray串接，NHWC to NCHW\n",
    "        self.data = np.vstack(self.data)\n",
    "        #列表ndarray串接\n",
    "        self.targets = np.vstack(self.targets)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def get_data_cifar10(self,num):\n",
    "        \"\"\"\n",
    "        dir_path = D:\\ml_data\\data_set\\cifar-10-python.tar\\augment_data\n",
    "        從dir_path讀取cifar資料，返回X,Y兩個numpy數組。\n",
    "        共0~5組資料，參數輸入num返回第num組資料。\n",
    "        \"\"\"\n",
    "        data_dir = r'D:\\ml_data\\data_set\\cifar-10-python.tar\\augment_data'\n",
    "        data_file_name_x = 'feature_{}.npy'.format(num)\n",
    "        data_file_name_y = 'label_{}.npy'.format(num)\n",
    "        data_path_x = os.path.join(data_dir,data_file_name_x)\n",
    "        data_path_y = os.path.join(data_dir,data_file_name_y)\n",
    "        X = np.load(data_path_x)\n",
    "        Y = np.load(data_path_y)\n",
    "        return X,Y\n",
    "\n",
    "def my_transforms(input):\n",
    "    output = torch.tensor(input,device=\"cuda:0\",dtype=torch.float32,requires_grad=False)\n",
    "    return output\n",
    "\n",
    "data = Custom_Cifar10(transform=my_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(data, batch_size=500,shuffle=True, num_workers=0)\n",
    "data_iter = iter(trainloader)\n",
    "x_batch = next(data_iter)[0]\n",
    "y_batch = next(data_iter)[1]\n",
    "\n",
    "print('origin x_size:',data.data.shape,'\\t x_size:',x_batch.shape)\n",
    "print('origin y_size:',data.targets.shape,'\\t\\t y_size:',y_batch.shape)\n",
    "print('x type:',type(x_batch),'\\t\\t y type:',type(y_batch))\n",
    "print('x device:',x_batch.device,'\\t\\t\\t x_dtype:',x_batch.dtype)\n",
    "print('getitem中x藉由my_transforms轉換成tensor，y依然是numpy，但生成iter時自動轉成tensor')\n",
    "```\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-78fd0f039c0e1552.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "## 數據並行 ##\n",
    "\n",
    "[Data Parallelism](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html#sphx-glr-beginner-blitz-data-parallel-tutorial-py)\n",
    "\n",
    "pytorch支持多GPU運行，設置也挺方便的，因為沒有多個GPU可以做範例QQ，請參考官方教程文檔。\n",
    "\n",
    "\n",
    "## 保存與讀取model ##\n",
    "[serialization](https://pytorch.org/docs/stable/torch.html#serialization)\n",
    "\n",
    "[Saving and Loading Models](https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
    "\n",
    "\n",
    "- 範例1：\n",
    "\n",
    ">model、optim都有state_dict()可以傳回有序字典，可以使用load_state_dict載入。\n",
    "\n",
    "```\n",
    "# Define model\n",
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = TheModelClass()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "```\n",
    "\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-65cbc841ae36ba74.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    ">保存\n",
    "\n",
    "```\n",
    "torch.save(model.state_dict(), PATH)\n",
    "```\n",
    "\n",
    ">加載\n",
    "\n",
    "```\n",
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "- 範例2：直接存取變量    \n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-db01ef6ae56960cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "- 範例3：\n",
    ">存於GPU由CPU加載，跨設備加載可以用這類方法。\n",
    "\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-3bc57706b97b51e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "- 範例4：\n",
    ">保存、加載整個模型，以這種方式保存模型將使用Python的[pickle](https://docs.python.org/3/library/pickle.html)模塊保存整個 模塊。\n",
    "\n",
    "![](https://upload-images.jianshu.io/upload_images/13539817-1d86c1ec70edd57b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
